---
title: "week12"
author: "Evan Knep"
date: "2024-04-21"
output: html_document
---

```{r}
# Script Settings And Resources
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(RedditExtractoR)
library(tm)
library(qdap)
library(SnowballC)
library(RWeka)
library(ldatuning)
library(wordcloud)
library(topicmodels)
library(tidytext)
```

# Data Import and Cleaning

```{r}
# urls <- find_thread_urls(subreddit = "IOPsychology", sort_by = "top", period = "year")
# reddit <- get_thread_content(urls$url)
# week12_tbl <- as_tibble(reddit$threads) %>%
#   select(c('title', 'upvotes'))
# write_csv(week12_tbl, "../data/reddit_IO_data.csv")
week12_tbl <- read_csv("../data/reddit_IO_data.csv")
week12_tbl$title
io_corpus_original <- VCorpus(VectorSource(week12_tbl$title))

  
```


```{r preprocessing}
io_corpus <- io_corpus_original %>%
  tm_map(content_transformer(str_to_lower)) %>% 
  tm_map(removeWords, c("io", "psychology", "psych", "i/o", "r/iopsychology")) %>%   
  tm_map(content_transformer(replace_abbreviation)) %>% 
  tm_map(content_transformer(replace_contraction)) %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeWords, stopwords("en")) %>%
  tm_map(stripWhitespace) %>%
  tm_map(stemDocument, language = "english")
  
  
```

*My corpus pre-processing steps went as follows:*
1. str_to_lower. Doing this first because then when I filter out words I don't need to worry about capitalization of each word being treated differently
2. removeWords. Removing all words related to IO psychology. Started with the basics: io, psychology, psych (common abbreviation), and then added i/o and riopsycholog (originally r/IOpsychology) when I saw it pop up in my scan of the corpus
3. replace_abbreviations. This and the following contraction replacement just seemed like good practice in any NLP. Maybe it'd be better practice to do this before removing words but I figured it didn't matter since there are no obvious abbreviations for IO psychology that I hadn't already selected for.
4. replace_contractions. Same justification as above.
5-8. removing numbers, punctuation, and basic english "stopwords" that are not meaningful for our analyses. Stripping whitespace at the end of sentences because that will still be read in otherwise
9. stemming document. Reducing words to there "stems" so that we don't need to worry about things like pluralization or varying word endings skewing our analyses.


# Analysis

```{r}

compare_them <- function() {
  casenum <- sample(1:40, 1)
  print(io_corpus[[casenum]]$content)
  print(io_corpus_original[[casenum]]$content)
}


```


```{r n-gram_DTM}
#Used RWeka to get uni and bigram tokens
myTokenizer <- function(x) {NGramTokenizer(x, Weka_control(min=1, max=2))} 
io_dtm <- DocumentTermMatrix(io_corpus,
                             control = list(tokenize = myTokenizer))

# io_dtm %>% as.matrix %>% as.tibble %>% View

io_slim_dtm <- removeSparseTerms(io_dtm, .9965)
tokenCounts <- apply(io_slim_dtm, 1, sum) 
io_slim_dtm <- io_slim_dtm[tokenCounts >0,] #ended up needing this after all, because rows of zeros were throwing errors in my derichlet allocation

io_slim_dtm %>% as.matrix %>% as.tibble %>% View

```


```{r dirichlet allocation}
dtm_tune <- FindTopicsNumber(
  io_slim_dtm,
  topics = seq(2,10,1),
  metrics = c(
    "Griffiths2004",
    "CaoJuan2009",
    "Arun2010",
    "Deveaud2014"),
  verbose = T
  )

FindTopicsNumber_plot(dtm_tune) #Early plotting of our LDA makes it seem likely that there are 4-5 distinct topics

lda_results <- LDA(io_slim_dtm, 4)
lda_betas <- tidy(lda_results, matrix = "beta") %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(topic, -beta) %>%
  View

lda_gammas <- tidy(lda_results, matrix = "gamma")


lda_gammas <- lda_gammas %>%
  group_by(document) %>%
  top_n(1, gamma) %>%
  slice(1) %>%
  ungroup %>%
  mutate(document = as.numeric(document)) %>%
  arrange(document)

```



```{r}
tidy_og_corpus <- tidy(io_corpus_original) %>%
  select(id, text) %>%
  rename(document = id) %>%
  mutate(across(document, as.integer))

glimpse(tidy_og_corpus)

topics_tbl <- left_join(lda_gammas, tidy_og_corpus, by = "document") %>%
  rename(doc_id = document, original = text, probability = gamma)
  
```

Looking at beta matrix alone we can get an idea of what our 4 topics are. Topic 1 seems to be related to career help in the form of getting a job, hence the interview questions portion. Topic 2 seems to be related to questions about best practice within the field. Topic 3 seems more related to graduate school/programs in IO psych. Topic 4 is a bit harder to pin down, but my interpretation would be that it captures the more general discussion posts. The subreddit seems to have a bi-weekly discussion for instance, and that is in topic 4. 

When looking at the topics matched up with the original post titles I would definitely say it feels a bit muddier,but that makes a good deal of sense considering that the probability of each post being in assigned topic is about 25%. There seems to be a great deal of overlap between topics, especially with many relating to questions about grad school and work, but the general framing of the questions does appear to differ somewhat (in my totally unbiased opinion).

```{r word cloud}

io_tbl <- io_slim_dtm %>% 
  as.matrix() %>%
  as.tibble()
wordcloud(
  words = names(io_tbl),
  freq = colSums(io_tbl),
  colors = brewer.pal(9,"RdPu")
)


```

I was a bit worried that the stemmed words would make the wordcloud look strange, but I think it actually turned out just fine


```{r final_tbl}
tidy_og_corpus <- tidy(io_corpus_original) %>%
  select(id, text) %>%
  rename(document = id) %>%
  mutate(across(document, as.integer))

week12_tbl <- week12_tbl %>%
  rename(original = title)

final_tbl <- left_join(topics_tbl, week12_tbl, by = "original")

```









