---
title: "week12"
author: "Evan Knep"
date: "2024-04-21"
output: html_document
---

```{r}
# Script Settings And Resources
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(RedditExtractoR)
library(tm)
library(qdap)
library(SnowballC)
library(RWeka)
library(ldatuning)
library(wordcloud)
library(topicmodels)
library(tidytext)
library(psych)
library(caret)
library(parallel)
library(doParallel)
```

# Data Import and Cleaning

```{r}
# urls <- find_thread_urls(subreddit = "IOPsychology", sort_by = "top", period = "year")
# reddit <- get_thread_content(urls$url)
# week12_tbl <- as_tibble(reddit$threads) %>%
#   select(c('title', 'upvotes'))
# write_csv(week12_tbl, "../data/reddit_IO_data.csv")
week12_tbl <- read_csv("../data/reddit_IO_data.csv")
week12_tbl$title
io_corpus_original <- VCorpus(VectorSource(week12_tbl$title))

  
```


```{r preprocessing}
io_corpus <- io_corpus_original %>%
  tm_map(content_transformer(str_to_lower)) %>% 
  tm_map(removeWords, c("io", "psychology", "psych", "i/o", "r/iopsychology")) %>%   
  tm_map(content_transformer(replace_abbreviation)) %>% 
  tm_map(content_transformer(replace_contraction)) %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeWords, stopwords("en")) %>%
  tm_map(stripWhitespace) %>%
  tm_map(stemDocument, language = "english")
  
  
```

*My corpus pre-processing steps went as follows:*
1. str_to_lower. Doing this first because then when I filter out words I don't need to worry about capitalization of each word being treated differently
2. removeWords. Removing all words related to IO psychology. Started with the basics: io, psychology, psych (common abbreviation), and then added i/o and riopsycholog (originally r/IOpsychology) when I saw it pop up in my scan of the corpus
3. replace_abbreviations. This and the following contraction replacement just seemed like good practice in any NLP. Maybe it'd be better practice to do this before removing words but I figured it didn't matter since there are no obvious abbreviations for IO psychology that I hadn't already selected for.
4. replace_contractions. Same justification as above.
5-8. removing numbers, punctuation, and basic english "stopwords" that are not meaningful for our analyses. Stripping whitespace at the end of sentences because that will still be read in otherwise
9. stemming document. Reducing words to there "stems" so that we don't need to worry about things like pluralization or varying word endings skewing our analyses.


# Analysis

```{r}

compare_them <- function() {
  casenum <- sample(1:40, 1)
  print(io_corpus[[casenum]]$content)
  print(io_corpus_original[[casenum]]$content)
}


```


```{r n-gram_DTM}
#Used RWeka to get uni and bigram tokens
myTokenizer <- function(x) {NGramTokenizer(x, Weka_control(min=1, max=2))} 
io_dtm <- DocumentTermMatrix(io_corpus,
                             control = list(tokenize = myTokenizer))

# io_dtm %>% as.matrix %>% as.tibble %>% View

io_slim_dtm <- removeSparseTerms(io_dtm, .9965)
tokenCounts <- apply(io_slim_dtm, 1, sum) 
io_slim_dtm <- io_slim_dtm[tokenCounts >0,] #ended up needing this after all, because rows of zeros were throwing errors in my derichlet allocation

# io_slim_dtm %>% as.matrix %>% as.tibble %>% View

```


```{r dirichlet allocation}
dtm_tune <- FindTopicsNumber(
  io_slim_dtm,
  topics = seq(2,10,1),
  metrics = c(
    "Griffiths2004",
    "CaoJuan2009",
    "Arun2010",
    "Deveaud2014"),
  verbose = T
  )

FindTopicsNumber_plot(dtm_tune) #Early plotting of our LDA makes it seem likely that there are 4-5 distinct topics

lda_results <- LDA(io_slim_dtm, 4)
lda_betas <- tidy(lda_results, matrix = "beta") %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(topic, -beta)

lda_gammas <- tidy(lda_results, matrix = "gamma")


lda_gammas <- lda_gammas %>%
  group_by(document) %>%
  top_n(1, gamma) %>%
  slice(1) %>%
  ungroup %>%
  mutate(document = as.numeric(document)) %>%
  arrange(document)

```



```{r}
tidy_og_corpus <- tidy(io_corpus_original) %>%
  select(id, text) %>%
  rename(document = id) %>%
  mutate(across(document, as.integer))

glimpse(tidy_og_corpus)

topics_tbl <- left_join(lda_gammas, tidy_og_corpus, by = "document") %>%
  rename(doc_id = document, original = text, probability = gamma)
  
```

Looking at beta matrix alone we can get an idea of what our 4 topics are. Topic 1 seems to be related to career help in the form of getting a job, hence the interview questions portion. Topic 2 seems to be related to questions about best practice within the field. Topic 3 seems more related to graduate school/programs in IO psych. Topic 4 is a bit harder to pin down, but my interpretation would be that it captures the more general discussion posts. The subreddit seems to have a bi-weekly discussion for instance, and that is in topic 4. 

When looking at the topics matched up with the original post titles I would definitely say it feels a bit muddier,but that makes a good deal of sense considering that the probability of each post being in assigned topic is about 25%. There seems to be a great deal of overlap between topics, especially with many relating to questions about grad school and work, but the general framing of the questions does appear to differ somewhat (in my totally unbiased opinion).

```{r word cloud}

io_tbl <- io_slim_dtm %>% 
  as.matrix() %>%
  as.tibble()
wordcloud(
  words = names(io_tbl),
  freq = colSums(io_tbl),
  colors = brewer.pal(9,"RdPu")
)


```

I was a bit worried that the stemmed words would make the wordcloud look strange, but I think it actually turned out just fine


```{r final_tbl}
tidy_og_corpus <- tidy(io_corpus_original) %>%
  select(id, text) %>%
  rename(document = id) %>%
  mutate(across(document, as.integer))

week12_tbl <- week12_tbl %>%
  rename(original = title)

final_tbl <- left_join(topics_tbl, week12_tbl, by = "original")

```


```{r dummy coding}
dummies <- dummy.code(final_tbl$topic)
dummies_tbl <- as.tibble(dummies)
names(dummies_tbl) <- c("topic1", "topic2", "topic3", "topic4")
final_tbl_dummied <- final_tbl %>%
  bind_cols(dummies_tbl)

```

```{r lm}
lm_1 <- lm (upvotes ~ topic1 + topic2 + topic3 + topic4, data = final_tbl_dummied)
summary(lm_1)

```
Interestingly, our linear regression suggests that only topic1 is a significant predictor of upvote, with a significant negative correlation (p = 2.16e-07) between topic 1 and post upvotes. If our interpretation of the topics is accurate, then this would indicate that the posts that are asking about jobs or interview advice are less popular, which would make sense if the sub is meant to be dedicated to more of a best practices or state of the field discussion.


```{r knn}

set.seed(500)
holdout_indices <- createDataPartition(final_tbl_dummied$upvotes,
                                       p = .25,
                                       list = T)$Resample1

training_tbl <- final_tbl_dummied[holdout_indices,]
test_tbl <- final_tbl_dummied[-holdout_indices,]

training_folds <- createFolds(training_tbl$upvotes)

local_cluster <- makeCluster(7)
registerDoParallel(local_cluster)

xgb_upvotes <- train(
  upvotes ~ topic,
  training_tbl,
  method = "xgbLinear",
  na.action = na.pass,
  preProcess = c("center","scale", "nzv", "medianImpute"),
  trControl = trainControl(method = "cv", 
                           number = 10, 
                           verboseIter = TRUE, 
                           indexOut = training_folds))



cv_xgb <- max(xgb_upvotes$results$Rsquared)
holdout_xgb <- cor(
  predict(xgb_upvotes, test_tbl, na.action = na.pass),
  test_tbl$upvotes
) ^ 2

cv_xgb
holdout_xgb

stopCluster(local_cluster)
registerDoSEQ()

```

When attempting to use xgboost to examine the relationship between topic and upvotes, our results suggest that we cannot predict upvotes based on topic. Our cv value is `r cv_xgb` and holdout prediction is `r holdout_xgb`.

