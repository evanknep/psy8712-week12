---
title: "week12"
author: "Evan Knep"
date: "2024-04-21"
output: html_document
---

```{r}
# Script Settings And Resources
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(RedditExtractoR)
library(tm)
library(qdap)
library(SnowballC)
library(RWeka)
library(ldatuning)
library(wordcloud)
```

# Data Import and Cleaning

```{r}
# urls <- find_thread_urls(subreddit = "IOPsychology", sort_by = "top", period = "year")
# reddit <- get_thread_content(urls$url)
# week12_tbl <- as_tibble(reddit$threads) %>%
#   select(c('title', 'upvotes'))
# write_csv(week12_tbl, "../data/reddit_IO_data.csv")
week12_tbl <- read_csv("../data/reddit_IO_data.csv")
week12_tbl$title
io_corpus_original <- VCorpus(VectorSource(week12_tbl$title))

  
```


```{r preprocessing}
io_corpus <- io_corpus_original %>%
  tm_map(content_transformer(str_to_lower)) %>% 
  tm_map(removeWords, c("io", "psychology", "psych", "i/o")) %>%   
  tm_map(content_transformer(replace_abbreviation)) %>% 
  tm_map(content_transformer(replace_contraction)) %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeWords, stopwords("en")) %>%
  tm_map(stripWhitespace) %>%
  tm_map(stemDocument, language = "english")
  
  
```

*My corpus pre-processing steps went as follows:*
1. str_to_lower. Doing this first because then when I filter out words I don't need to worry about capitalization of each word being treated differently
2. removeWords. Removing all words related to IO psychology. Started with the basics: io, psychology, psych (common abbreviation), and then added i/o when I saw it pop up in my scan of the corpus
3. replace_abbreviations. This and the following contraction replacement just seemed like good practice in any NLP. Maybe it'd be better practice to do this before removing words but I figured it didn't matter since there are no obvious abbreviations for IO psychology that I hadn't already selected for.
4. replace_contractions. Same justification as above.
5-8. removing numbers, punctuation, and basic english "stopwords" that are not meaningful for our analyses. Stripping whitespace at the end of sentences because that will still be read in otherwise
9. stemming document. Reducing words to there "stems" so that we don't need to worry about things like pluralization or varying word endings skewing our analyses.


# Analysis

```{r}

compare_them <- function() {
  casenum <- sample(1:40, 1)
  print(io_corpus[[casenum]]$content)
  print(io_corpus_original[[casenum]]$content)
}


```


```{r n-gram_DTM}
#Used RWeka to get uni and bigram tokens
myTokenizer <- function(x) {NGramTokenizer(x, Weka_control(min=1, max=2))} 
io_dtm <- DocumentTermMatrix(io_corpus,
                             control = list(tokenize = myTokenizer))

# io_dtm %>% as.matrix %>% as.tibble %>% View

io_slim_dtm <- removeSparseTerms(io_dtm, .9965)
tokenCounts <- apply(io_slim_dtm, 1, sum) 
io_slim_dtm <- io_slim_dtm[tokenCounts >0,] #ended up needing this after all, because rows of zeros were throwing errors in my derichlet allocation

# io_slim_dtm %>% as.matrix %>% as.tibble %>% View

```


```{r dirichlet allocation}
topics <- FindTopicsNumber(
  io_slim_dtm,
  topics = seq(2,10,1),
  metrics = c(
    "Griffiths2004",
    "CaoJuan2009",
    "Arun2010",
    "Deveaud2014"),
  verbose = T
  )

FindTopicsNumber_plot(topics) #Early plotting of our LDA makes it seem likely that there are 4-5 distinct topics

```





